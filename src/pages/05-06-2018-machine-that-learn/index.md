---
path: "/another-one"
date: "2018-06-05T17:12:33.962Z"
title: "Learning to Trust Machines That Learn"
---
Imagine lying on a hospital bed. Doctors with grave expressions hover. One leans down to tell you that you are terribly sick and says they recommend a risky procedure as your best hope. You ask them to explain what’s going on. They cannot. Your trust in the doctors ebbs away.

Replace the doctors with a computer program and you more or less have the state of artificial intelligence (AI) today. Technology increasingly insinuates itself into our lives, affecting the decisions we make and the decisions others make for us. More and more, we give computers responsibility and autonomy to decide on life-changing events. If your hospital bed happens to be in the Memorial Sloan Kettering Cancer Center, for example, your oncologist might have asked IBM’s Watson for advice. Watson is a computer system that can answer questions posed in everyday language, most famously using its skills to win the TV quiz show Jeopardy!in 2011.

The problem is, Watson cannot tell you why it decided you have cancer. Machines are currently incapable of explaining their decisions. And as J.K. Rowling wrote in Harry Potter and the Chamber of Secrets, “Never trust anything that can think for itself if you can’t see where it keeps its brain.”

Some AI researchers are attempting to address this by creating programs that can essentially keep an eye on their own decision-making processes and describe how they figured things out (known as “explainable AI”). But there is a deeper question: Is explainable AI important, or is it merely a remedy for our own discomfort?

We must choose to trust or distrust the machines. Just as we learn to trust other humans (or not), this is likely to be a convoluted, multifaceted, and at times fraught process. Trust emerges not only from the outcomes of any given interaction but also through expertise, context, experience, and emotions.

Anthropologists interested in the scientific study of trust have embraced game theory for this very purpose. It all started, appropriately enough, with computers.

In game theory, situations in which you are affected by the behavior of others are known as games. Game theory gives us a way of calculating how these social interactions might proceed; we do so by looking at how an individual following a particular strategy — rules of conduct in certain circumstances — fares depending on the strategies of others. In the early 1980s, political scientist Robert Axelrod hosted a tournament of games in his computer to explore which strategies won out in the long run. He found that a copycat, tit-for-tat strategy of initially being cooperative then simply copying your social partner’s behavior did best, even against utterly selfish strategies.

In an effort to unravel human behavior, social scientists took these abstract mathematical games into the lab. They looked at how people, typically students, responded to incentives such as money, course credit, or pizza. Students, though, are not very representative compared to the majority of humans: They tend to be WEIRD, or westernized, educated, industrialized, rich, and democratic. So anthropologists took these games into the field.